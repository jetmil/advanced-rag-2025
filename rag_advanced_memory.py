"""
RAG —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –ø–∞–º—è—Ç—å—é –∏ –∞–≤—Ç–æ—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π
–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –ø–æ–¥ RTX 3090 (24GB VRAM) + 32GB RAM
"""

from rag_knowledge_base import LocalRAG
from typing import List, Dict, Optional
import tiktoken
from datetime import datetime
import re

class AdvancedRAGMemory(LocalRAG):
    """
    RAG —Å —É–º–Ω–æ–π –ø–∞–º—è—Ç—å—é –∏ –∞–≤—Ç–æ—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–µ–π

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω—ã –ø–æ–¥:
    - RTX 3090 (24GB VRAM)
    - 32GB RAM
    - Gemma-27B (context: 8192 tokens)
    """

    def __init__(
        self,
        *args,
        max_short_memory: int = 5,           # –ü–æ—Å–ª–µ–¥–Ω–∏–µ N —Å–æ–æ–±—â–µ–Ω–∏–π –≤ –ø–æ–ª–Ω–æ–º –≤–∏–¥–µ
        max_context_tokens: int = 6000,      # –ú–∞–∫—Å —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ (—Å –∑–∞–ø–∞—Å–æ–º)
        summarize_threshold: int = 4000,     # –ü–æ—Ä–æ–≥ –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        enable_auto_summarize: bool = True,  # –ê–≤—Ç–æ—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        **kwargs
    ):
        super().__init__(*args, **kwargs)

        # –ü–∞–º—è—Ç—å
        self.short_memory: List[Dict] = []      # –ü–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è (–ø–æ–ª–Ω—ã–µ)
        self.long_memory: List[str] = []        # –°—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è
        self.session_start = datetime.now()

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
        self.max_short_memory = max_short_memory
        self.max_context_tokens = max_context_tokens
        self.summarize_threshold = summarize_threshold
        self.enable_auto_summarize = enable_auto_summarize

        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –¥–ª—è –ø–æ–¥—Å—á–µ—Ç–∞
        try:
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        except:
            self.tokenizer = None
            print("‚ö†Ô∏è tiktoken –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –ø–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤ –ø—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω—ã–π")

    def _count_tokens(self, text: str) -> int:
        """–ü–æ–¥—Å—á–µ—Ç —Ç–æ–∫–µ–Ω–æ–≤"""
        if self.tokenizer:
            return len(self.tokenizer.encode(text))
        else:
            # –ü—Ä–∏–±–ª–∏–∑–∏—Ç–µ–ª—å–Ω–æ: 1 —Ç–æ–∫–µ–Ω ‚âà 4 —Å–∏–º–≤–æ–ª–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ
            return len(text) // 4

    def hybrid_search(self, query: str, k: int = 10, keyword_boost: float = 4.0):
        """
        –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫: –≤–µ–∫—Ç–æ—Ä–Ω—ã–π + keyword —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è

        1. –í–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫ (MMR) - –Ω–∞—Ö–æ–¥–∏—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏ –ø–æ—Ö–æ–∂–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã
        2. Keyword —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è - –ø—Ä–æ–≤–µ—Ä—è–µ—Ç –Ω–∞–ª–∏—á–∏–µ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤ –∏–∑ –∑–∞–ø—Ä–æ—Å–∞
        3. –ë—É—Å—Ç–∏–Ω–≥ - –ø–æ–≤—ã—à–∞–µ—Ç —Ä–∞–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ç–æ—á–Ω—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º

        Args:
            query: –ø–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
            k: –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            keyword_boost: –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç —É—Å–∏–ª–µ–Ω–∏—è –¥–ª—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å —Ç–æ—á–Ω—ã–º —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ–º
        """
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –ó–ù–ê–ß–ò–ú–´–ï –°–õ–û–í–ê (—Ä—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞ >=4 —Å–∏–º–≤–æ–ª–∞)
        # –ò–°–ö–õ–Æ–ß–ê–ï–ú —Å–ª—É–∂–µ–±–Ω—ã–µ —Å–ª–æ–≤–∞ –∏ –∫–æ—Ä–æ—Ç–∫–∏–µ –ø—Ä–µ–¥–ª–æ–≥–∏
        stopwords = {'—á—Ç–æ', '–∫–∞–∫', '–≥–¥–µ', '–∫–æ–≥–¥–∞', '–∑–∞—á–µ–º', '–ø–æ—á–µ–º—É', '–∫–∞–∫–æ–π', '–∫–∞–∫–∞—è', '–∫–∞–∫–∏–µ', '–∫–æ—Ç–æ—Ä—ã–π', '–∫–æ—Ç–æ—Ä–∞—è', '–∫–æ—Ç–æ—Ä—ã–µ', '—ç—Ç–æ—Ç', '—ç—Ç–∞', '—ç—Ç–æ', '—ç—Ç–∏', '—Ç–æ–≥–æ', '—Ç–æ–º—É', '—ç—Ç–æ–≥–æ', '–æ–±—â–µ–≥–æ'}
        keywords = []
        # –ò—â–µ–º –í–°–ï —Ä—É—Å—Å–∫–∏–µ —Å–ª–æ–≤–∞ –¥–ª–∏–Ω–æ–π >=4 —Å–∏–º–≤–æ–ª–∞ (–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ –æ—Ç —Ä–µ–≥–∏—Å—Ç—Ä–∞)
        words = re.findall(r'\b[–∞-—è—ë–ê-–Ø–Å]{4,}\b', query.lower())
        keywords = [w.capitalize() for w in words if w not in stopwords]

        # 1. –í–ï–ö–¢–û–†–ù–´–ô –ü–û–ò–°–ö (MMR) - –≤—Å–µ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑—É–µ–º, —Ç–∞–∫ –∫–∞–∫ ChromaDB direct search —Å–ª–∏—à–∫–æ–º –º–µ–¥–ª–µ–Ω–Ω—ã–π
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º k –¥–ª—è –ª—É—á—à–µ–≥–æ –æ—Ö–≤–∞—Ç–∞ –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
        if keywords:
            # –ë–æ–ª—å—à–µ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è keyword —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
            vector_docs = self.vectorstore.max_marginal_relevance_search(
                query, k=k * 5, fetch_k=k * 15, lambda_mult=0.3
            )
        else:
            # –û–±—ã—á–Ω—ã–π –≤–µ–∫—Ç–æ—Ä–Ω—ã–π –ø–æ–∏—Å–∫
            vector_docs = self.vectorstore.max_marginal_relevance_search(
                query, k=k * 3, fetch_k=k * 9, lambda_mult=0.5
            )

        # 2. Keyword —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∏ —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏–µ
        scored_docs = []
        for doc in vector_docs:
            content = doc.page_content
            score = 1.0  # –ë–∞–∑–æ–≤—ã–π —Å–∫–æ—Ä –æ—Ç –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞

            # –ü–æ–¥—Å—á–µ—Ç —Å–æ–≤–ø–∞–¥–µ–Ω–∏–π –∫–ª—é—á–µ–≤—ã—Ö —Å–ª–æ–≤
            matches = 0
            for keyword in keywords:
                if keyword in content or keyword.lower() in content.lower():
                    matches += 1

            # –ë—É—Å—Ç–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∫–ª—é—á–µ–≤—ã–º–∏ —Å–ª–æ–≤–∞–º–∏
            if matches > 0:
                score = score * (keyword_boost ** matches)

            scored_docs.append((score, doc, matches))

        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Å–∫–æ—Ä—É
        scored_docs.sort(key=lambda x: x[0], reverse=True)

        # –í–æ–∑–≤—Ä–∞—Ç —Ç–æ–ø-k –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        result_docs = [doc for _, doc, _ in scored_docs[:k]]

        return result_docs

    def _summarize_old_messages(self) -> str:
        """–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ä—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π"""
        if len(self.short_memory) < 3:
            return None

        # –ë–µ—Ä–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏ (–∫—Ä–æ–º–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 2)
        messages_to_summarize = self.short_memory[:-2]

        if not messages_to_summarize:
            return None

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ç–µ–∫—Å—Ç –¥–ª—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏
        dialogue = ""
        for msg in messages_to_summarize:
            dialogue += f"Q: {msg['question']}\nA: {msg['answer'][:300]}...\n\n"

        # –ó–∞–ø—Ä–æ—Å –Ω–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—é
        summary_prompt = f"""–°–¥–µ–ª–∞–π –∫—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ —Å–ª–µ–¥—É—é—â–µ–≥–æ –¥–∏–∞–ª–æ–≥–∞ (2-3 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è):

{dialogue}

–ö—Ä–∞—Ç–∫–æ–µ —Ä–µ–∑—é–º–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö —Ç–µ–º –∏ –≤—ã–≤–æ–¥–æ–≤:"""

        try:
            response = self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "–¢—ã —Å—É–º–º–∞—Ä–∏–∑–∏—Ä—É–µ—à—å –¥–∏–∞–ª–æ–≥–∏ –∫—Ä–∞—Ç–∫–æ –∏ —Ç–æ—á–Ω–æ."},
                    {"role": "user", "content": summary_prompt}
                ],
                max_tokens=200,
                temperature=0.3
            )

            summary = response.choices[0].message.content

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –¥–æ–ª–≥—É—é –ø–∞–º—è—Ç—å
            self.long_memory.append(summary)

            # –£–¥–∞–ª—è–µ–º —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
            self.short_memory = self.short_memory[-2:]

            return summary

        except Exception as e:
            print(f"‚ö†Ô∏è –û—à–∏–±–∫–∞ —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏–∏: {e}")
            return None

    def _format_memory_for_prompt(self, question: str, context: str) -> tuple:
        """
        –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ø–∞–º—è—Ç–∏
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (prompt, tokens_used)
        """

        # –ë–∞–∑–æ–≤—ã–π –ø—Ä–æ–º–ø—Ç
        base_prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–æ—Å–º–æ—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–µ –∏ —ç–∑–æ—Ç–µ—Ä–∏—á–µ—Å–∫–∏–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º.

–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∫–∞–∫ –û–°–ù–û–í–£ –¥–ª—è –æ—Ç–≤–µ—Ç–∞.
–ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ - –æ–ø–∏—Ä–∞–π—Å—è –Ω–∞ –Ω–µ–≥–æ –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å.
–ù–æ —Ç–∞–∫–∂–µ –º–æ–∂–µ—à—å –¥–æ–ø–æ–ª–Ω—è—Ç—å –æ—Ç–≤–µ—Ç —Å–≤–æ–∏–º–∏ –æ–±—â–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏, –µ—Å–ª–∏:
- –ö–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
- –ù—É–∂–Ω–æ –æ–±—ä—è—Å–Ω–∏—Ç—å –æ–±—â–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–ª–∏ —Ç–µ—Ä–º–∏–Ω—ã
- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ —á–µ–º-—Ç–æ –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

–¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}

–ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç:"""

        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã –¥–ª—è –±–∞–∑–æ–≤–æ–≥–æ –ø—Ä–æ–º–ø—Ç–∞ –∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
        base_tokens = self._count_tokens(base_prompt)
        context_tokens = self._count_tokens(f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context}\n\n")

        available_for_memory = self.max_context_tokens - base_tokens - context_tokens - 500  # –∑–∞–ø–∞—Å

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∏—Å—Ç–æ—Ä–∏—é
        memory_text = ""

        # –î–æ–±–∞–≤–ª—è–µ–º –¥–æ–ª–≥—É—é –ø–∞–º—è—Ç—å (—Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é)
        if self.long_memory:
            long_mem = "\n–ü—Ä–µ–¥—ã–¥—É—â–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞:\n" + "\n".join(self.long_memory[-2:]) + "\n"
            long_tokens = self._count_tokens(long_mem)
            if long_tokens < available_for_memory:
                memory_text += long_mem
                available_for_memory -= long_tokens

        # –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–æ—Ç–∫—É—é –ø–∞–º—è—Ç—å (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è)
        if self.short_memory:
            recent_msgs = "\n–ü–æ—Å–ª–µ–¥–Ω–∏–µ –≤–æ–ø—Ä–æ—Å—ã:\n"
            for msg in reversed(self.short_memory):
                msg_text = f"Q: {msg['question']}\nA: {msg['answer'][:200]}...\n"
                msg_tokens = self._count_tokens(msg_text)

                if msg_tokens < available_for_memory:
                    recent_msgs = msg_text + recent_msgs
                    available_for_memory -= msg_tokens
                else:
                    break

            if recent_msgs != "\n–ü–æ—Å–ª–µ–¥–Ω–∏–µ –≤–æ–ø—Ä–æ—Å—ã:\n":
                memory_text += recent_msgs

        # –§–∏–Ω–∞–ª—å–Ω—ã–π –ø—Ä–æ–º–ø—Ç
        final_prompt = f"""–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–æ—Å–º–æ—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–µ –∏ —ç–∑–æ—Ç–µ—Ä–∏—á–µ—Å–∫–∏–º –ø—Ä–∞–∫—Ç–∏–∫–∞–º.

–í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π –∫–∞–∫ –û–°–ù–û–í–£ –¥–ª—è –æ—Ç–≤–µ—Ç–∞.
–ï—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–æ–¥–µ—Ä–∂–∏—Ç –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ - –æ–ø–∏—Ä–∞–π—Å—è –Ω–∞ –Ω–µ–≥–æ –≤ –ø–µ—Ä–≤—É—é –æ—á–µ—Ä–µ–¥—å.
–ù–æ —Ç–∞–∫–∂–µ –º–æ–∂–µ—à—å –¥–æ–ø–æ–ª–Ω—è—Ç—å –æ—Ç–≤–µ—Ç —Å–≤–æ–∏–º–∏ –æ–±—â–∏–º–∏ –∑–Ω–∞–Ω–∏—è–º–∏, –µ—Å–ª–∏:
- –ö–æ–Ω—Ç–µ–∫—Å—Ç–∞ –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è –ø–æ–ª–Ω–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
- –ù—É–∂–Ω–æ –æ–±—ä—è—Å–Ω–∏—Ç—å –æ–±—â–∏–µ –∫–æ–Ω—Ü–µ–ø—Ü–∏–∏ –∏–ª–∏ —Ç–µ—Ä–º–∏–Ω—ã
- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ —á–µ–º-—Ç–æ –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞

{memory_text}

–ö–æ–Ω—Ç–µ–∫—Å—Ç –∏–∑ –±–∞–∑—ã –∑–Ω–∞–Ω–∏–π:
{context}

–¢–µ–∫—É—â–∏–π –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è: {question}

–ü–æ–¥—Ä–æ–±–Ω—ã–π –æ—Ç–≤–µ—Ç:"""

        total_tokens = self._count_tokens(final_prompt)

        return final_prompt, total_tokens

    def query(
        self,
        question: str,
        max_tokens: int = 2000,
        temperature: float = 0.7,
        force_summarize: bool = False
    ) -> dict:
        """
        –ó–∞–ø—Ä–æ—Å —Å —É–º–Ω–æ–π –ø–∞–º—è—Ç—å—é

        Args:
            question: –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            max_tokens: –º–∞–∫—Å —Ç–æ–∫–µ–Ω–æ–≤ –æ—Ç–≤–µ—Ç–∞
            temperature: —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            force_summarize: –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        """

        if self.retriever is None:
            raise ValueError("QA chain not created.")

        # –ê–≤—Ç–æ—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if self.enable_auto_summarize and len(self.short_memory) >= self.max_short_memory:
            print("üîÑ –ê–≤—Ç–æ—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø–∞–º—è—Ç–∏...")
            summary = self._summarize_old_messages()
            if summary:
                print(f"‚úÖ –°–æ–∑–¥–∞–Ω–æ —Ä–µ–∑—é–º–µ: {summary[:100]}...")

        # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        if force_summarize and self.short_memory:
            print("üîÑ –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è...")
            self._summarize_old_messages()

        # –ü–æ–ª—É—á–µ–Ω–∏–µ —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —á–µ—Ä–µ–∑ –ì–ò–ë–†–ò–î–ù–´–ô –ü–û–ò–°–ö
        relevant_docs = self.hybrid_search(question, k=10)
        context = "\n\n".join([doc.page_content for doc in relevant_docs])

        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞ —Å –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–π –ø–∞–º—è—Ç—å—é
        prompt, tokens_used = self._format_memory_for_prompt(question, context)

        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–µ–≤—ã—à–µ–Ω–∏–µ –ª–∏–º–∏—Ç–∞
        if tokens_used > self.summarize_threshold and self.enable_auto_summarize:
            print("‚ö†Ô∏è –ü—Ä–µ–≤—ã—à–µ–Ω –ø–æ—Ä–æ–≥ —Ç–æ–∫–µ–Ω–æ–≤, —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è...")
            self._summarize_old_messages()
            # –ü–æ–≤—Ç–æ—Ä–Ω–æ–µ —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–æ–º–ø—Ç–∞
            prompt, tokens_used = self._format_memory_for_prompt(question, context)

        # –ó–∞–ø—Ä–æ—Å –∫ LLM
        try:
            response = self.llm_client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∫–æ—Å–º–æ—ç–Ω–µ—Ä–≥–µ—Ç–∏–∫–µ —Å –ø–∞–º—è—Ç—å—é –¥–∏–∞–ª–æ–≥–∞."},
                    {"role": "user", "content": prompt}
                ],
                max_tokens=max_tokens,
                temperature=temperature
            )

            answer = response.choices[0].message.content

            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ –∫–æ—Ä–æ—Ç–∫—É—é –ø–∞–º—è—Ç—å
            self.short_memory.append({
                "question": question,
                "answer": answer,
                "timestamp": datetime.now().isoformat()
            })

            return {
                "answer": answer,
                "source_documents": relevant_docs,
                "context": context,
                "memory_stats": {
                    "short_memory_size": len(self.short_memory),
                    "long_memory_size": len(self.long_memory),
                    "tokens_used": tokens_used,
                    "tokens_limit": self.max_context_tokens
                }
            }

        except Exception as e:
            return {
                "answer": f"–û—à–∏–±–∫–∞: {str(e)}\n\n–ü—Ä–æ–≤–µ—Ä—å—Ç–µ, —á—Ç–æ LM Studio –∑–∞–ø—É—â–µ–Ω!",
                "source_documents": relevant_docs,
                "context": context,
                "memory_stats": {
                    "short_memory_size": len(self.short_memory),
                    "long_memory_size": len(self.long_memory),
                    "tokens_used": tokens_used,
                    "tokens_limit": self.max_context_tokens
                }
            }

    def clear_memory(self, keep_summaries: bool = False):
        """–û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏"""
        self.short_memory = []
        if not keep_summaries:
            self.long_memory = []
        return f"–ü–∞–º—è—Ç—å –æ—á–∏—â–µ–Ω–∞. –°—É–º–º–∞—Ä–∏–∏ {'—Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã' if keep_summaries else '—É–¥–∞–ª–µ–Ω—ã'}."

    def get_memory_stats(self) -> dict:
        """–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏"""
        return {
            "session_duration": str(datetime.now() - self.session_start),
            "short_memory_count": len(self.short_memory),
            "long_memory_count": len(self.long_memory),
            "total_questions": len(self.short_memory) + sum(1 for _ in self.long_memory),
            "auto_summarize_enabled": self.enable_auto_summarize,
            "max_context_tokens": self.max_context_tokens,
            "summarize_threshold": self.summarize_threshold
        }

    def export_conversation(self, filepath: str):
        """–≠–∫—Å–ø–æ—Ä—Ç –≤—Å–µ–π –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞"""
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"–°–µ—Å—Å–∏—è –Ω–∞—á–∞—Ç–∞: {self.session_start}\n")
            f.write("="*70 + "\n\n")

            # –î–æ–ª–≥–∞—è –ø–∞–º—è—Ç—å
            if self.long_memory:
                f.write("–°–£–ú–ú–ê–†–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ò–°–¢–û–†–ò–Ø:\n")
                f.write("-"*70 + "\n")
                for i, summary in enumerate(self.long_memory, 1):
                    f.write(f"{i}. {summary}\n\n")
                f.write("\n")

            # –ö–æ—Ä–æ—Ç–∫–∞—è –ø–∞–º—è—Ç—å
            if self.short_memory:
                f.write("–ü–û–°–õ–ï–î–ù–ò–ï –°–û–û–ë–©–ï–ù–ò–Ø:\n")
                f.write("-"*70 + "\n")
                for i, msg in enumerate(self.short_memory, 1):
                    f.write(f"\n[{msg.get('timestamp', 'N/A')}]\n")
                    f.write(f"–í–æ–ø—Ä–æ—Å: {msg['question']}\n")
                    f.write(f"–û—Ç–≤–µ—Ç: {msg['answer']}\n")
                    f.write("-"*70 + "\n")

        return f"–ò—Å—Ç–æ—Ä–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤ {filepath}"


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    from pathlib import Path
    project_dir = Path(__file__).parent
    TEXT_FILE = str(project_dir / "cosmic_texts.txt")
    DB_PATH = str(project_dir / "chroma_db_kosmoenergy")

    print("="*70)
    print("RAG —Å –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–π –ø–∞–º—è—Ç—å—é")
    print("–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è RTX 3090 + 32GB RAM")
    print("="*70)

    rag = AdvancedRAGMemory(
        text_file_path=TEXT_FILE,
        db_path=DB_PATH,
        embedding_model="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        max_short_memory=5,          # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 5 —Å–æ–æ–±—â–µ–Ω–∏–π
        max_context_tokens=6000,     # –ú–∞–∫—Å 6000 —Ç–æ–∫–µ–Ω–æ–≤ (Gemma-27B context: 8192)
        summarize_threshold=4000,    # –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –ø—Ä–∏ 4000 —Ç–æ–∫–µ–Ω–æ–≤
        enable_auto_summarize=True,  # –ê–≤—Ç–æ—Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è
        use_gpu=True
    )

    # –ó–∞–≥—Ä—É–∑–∫–∞ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –ë–î
    from langchain_community.vectorstores import Chroma
    print("\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ–∫—Ç–æ—Ä–Ω–æ–π –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö...")
    rag.vectorstore = Chroma(
        persist_directory=DB_PATH,
        embedding_function=rag.embeddings
    )

    print("üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ LM Studio...")
    rag.setup_lm_studio_llm(model_name="google/gemma-3-27b")

    print("‚öôÔ∏è –ù–∞—Å—Ç—Ä–æ–π–∫–∞ retriever...")
    rag.create_qa_chain(retriever_k=4)

    print("\n" + "="*70)
    print("‚úÖ –°–∏—Å—Ç–µ–º–∞ –≥–æ—Ç–æ–≤–∞!")
    print("="*70)
    print("\n–ö–æ–º–∞–Ω–¥—ã:")
    print("  'clear' - –æ—á–∏—Å—Ç–∏—Ç—å –ø–∞–º—è—Ç—å (—Å–æ—Ö—Ä–∞–Ω–∏—Ç—å —Å—É–º–º–∞—Ä–∏–∏)")
    print("  'clear all' - –æ—á–∏—Å—Ç–∏—Ç—å –≤—Å—ë")
    print("  'stats' - —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏")
    print("  'export' - —ç–∫—Å–ø–æ—Ä—Ç –∏—Å—Ç–æ—Ä–∏–∏ –≤ —Ñ–∞–π–ª")
    print("  'summarize' - –ø—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è —Å—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è")
    print("  'quit' - –≤—ã—Ö–æ–¥")
    print("="*70 + "\n")

    while True:
        question = input("\nüí¨ –í–∞—à –≤–æ–ø—Ä–æ—Å: ").strip()

        if not question:
            continue

        if question.lower() in ['quit', 'exit', 'q']:
            print("\nüëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!")
            break

        if question.lower() == 'clear':
            print(rag.clear_memory(keep_summaries=True))
            continue

        if question.lower() == 'clear all':
            print(rag.clear_memory(keep_summaries=False))
            continue

        if question.lower() == 'stats':
            stats = rag.get_memory_stats()
            print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–∞–º—è—Ç–∏:")
            for key, value in stats.items():
                print(f"  {key}: {value}")
            continue

        if question.lower() == 'export':
            filename = f"conversation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
            filepath = str(project_dir / filename)
            print(rag.export_conversation(filepath))
            continue

        if question.lower() == 'summarize':
            result = rag.query(question, force_summarize=True)
            print("‚úÖ –°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
            continue

        # –û–±—ã—á–Ω—ã–π –∑–∞–ø—Ä–æ—Å
        print("\nüîç –ü–æ–∏—Å–∫ –≤ –±–∞–∑–µ –∑–Ω–∞–Ω–∏–π...")
        result = rag.query(question)

        print("\n" + "="*70)
        print("üìù –û–¢–í–ï–¢:")
        print("="*70)
        print(result['answer'])
        print("\n" + "="*70)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        stats = result['memory_stats']
        print(f"üíæ –ü–∞–º—è—Ç—å: {stats['short_memory_size']} –Ω–µ–¥–∞–≤–Ω–∏—Ö + {stats['long_memory_size']} —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö")
        print(f"üìä –¢–æ–∫–µ–Ω—ã: {stats['tokens_used']}/{stats['tokens_limit']}")
        print("="*70)

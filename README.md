# Advanced RAG 2025

Продвинутая система RAG (Retrieval-Augmented Generation) с умной памятью диалога и современным glassmorphism UI.

## Особенности

- **Умная память диалога** с двухуровневой архитектурой (короткая + долгая память)
- **Автоматическая суммаризация** истории при превышении порога токенов
- **GPU-ускорение** для embeddings (оптимизировано для RTX 3090)
- **Современный UI** с эффектами glassmorphism и CSS-анимациями
- **Мультибазы знаний** - поддержка нескольких векторных баз
- **Экспорт истории** разговоров

## Технологический стек

- **LangChain** - фреймворк для RAG
- **ChromaDB** - персистентная векторная база данных
- **Sentence Transformers** - embeddings на GPU
- **LM Studio** - локальный сервер LLM (OpenAI-совместимый API)
- **Gradio** - современный веб-интерфейс
- **PyTorch** - GPU-ускорение (CUDA 12.4)

## Системные требования

- **GPU:** NVIDIA RTX 3090 (24GB VRAM) или аналогичная
- **RAM:** 32GB
- **OS:** Windows 10/11
- **Python:** 3.13+
- **CUDA:** 12.4+

## Быстрый старт

### 1. Установка зависимостей

```bash
pip install -r requirements.txt
```

**Важно:** Для GPU-ускорения установите PyTorch с CUDA:
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
```

### 2. Запуск LM Studio

Вариант A - Через GUI:
1. Откройте LM Studio
2. Загрузите модель Gemma-3-27B
3. Перейдите в "Local Server"
4. Нажмите "Start Server" (порт 1234)

Вариант B - Через командную строку:
```bash
start_lmstudio_server.bat
```

Проверить статус:
```bash
check_lmstudio_status.bat
```

### 3. Запуск веб-интерфейса

```bash
python rag_web_modern.py
```

Откройте браузер: http://localhost:7860

## Использование

### Инициализация системы

1. В вкладке "Инициализация" укажите:
   - Путь к текстовому файлу с вашей базой знаний
   - Название базы данных
   - Настройки памяти (опционально)

2. Нажмите "Инициализировать систему"

3. Дождитесь завершения (~20 сек для загрузки, 2-3 мин для первого создания БД)

### Работа с чатом

1. Перейдите на вкладку "Чат"
2. Введите вопрос
3. Настройте параметры генерации (опционально):
   - **Temperature** (0.0-1.0): креативность ответов
   - **Max tokens** (500-4000): длина ответа
   - **Количество источников** (1-10): сколько фрагментов использовать

### Управление памятью

На вкладке "Управление памятью":
- Просмотр статистики сессии
- Очистка памяти (с сохранением/удалением суммаризации)
- Экспорт истории разговора

## Архитектура памяти

```
┌─────────────────────────────────────┐
│     Короткая память (5 сообщений)   │
│  Последние вопросы/ответы полностью │
└──────────────┬──────────────────────┘
               │
               ↓
┌─────────────────────────────────────┐
│    Долгая память (суммаризация)     │
│   LLM сжимает старые сообщения      │
└──────────────┬──────────────────────┘
               │
               ↓
┌─────────────────────────────────────┐
│     Формирование контекста          │
│   Умещается в 6000 токенов          │
│   (Gemma-3-27B: лимит 8192)         │
└─────────────────────────────────────┘
```

### Оптимизация для RTX 3090 + 32GB RAM

- **Короткая память:** 5 сообщений (полный текст)
- **Макс контекст:** 6000 токенов
- **Порог суммаризации:** 4000 токенов
- **Embedding на GPU:** Да
- **Резерв VRAM:** ~4GB для других задач

## Структура проекта

```
advanced-rag-2025/
├── rag_knowledge_base.py      # Базовый RAG класс
├── rag_advanced_memory.py     # RAG с умной памятью
├── rag_web_advanced.py        # Простой веб-интерфейс
├── rag_web_modern.py          # Современный UI с glassmorphism
├── start_lmstudio_server.bat  # Запуск LM Studio сервера
├── check_lmstudio_status.bat  # Проверка статуса
├── stop_lmstudio_server.bat   # Остановка сервера
├── requirements.txt           # Зависимости
├── .gitignore                 # Git ignore
├── README.md                  # Эта инструкция
├── RAG_PROJECT_README.md      # Полная документация
└── LM_STUDIO_SERVER_GUIDE.md  # Руководство по LM Studio
```

## Решение проблем

### Ошибка: "Connection refused"
- Убедитесь, что LM Studio запущен
- Проверьте, что сервер на порту 1234
- Запустите `check_lmstudio_status.bat`

### Ошибка: "CUDA not available"
- Переустановите PyTorch с CUDA:
  ```bash
  pip uninstall torch torchvision torchaudio -y
  pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124
  ```

### Медленная работа
- Закройте другие GPU-приложения
- Уменьшите количество источников
- Используйте квантованную модель в LM Studio

### База данных создается долго
- Это нормально для первого раза (2-3 минуты)
- Последующие загрузки мгновенны
- ChromaDB сохраняется в `chroma_db_*` папках

## История разработки

### Преодоленные проблемы

1. **Unicode в консоли** - удалены emoji из установочных скриптов
2. **Отсутствие зависимостей** - добавлены multiprocess, dill, xxhash
3. **PyTorch без CUDA** - переустановка с правильным индексом
4. **Конфигурация модели** - изменена с Qwen на Gemma-3-27B
5. **Производительность UI** - CSS-анимации вместо JavaScript
6. **Управление памятью** - двухуровневая архитектура с автосуммаризацией

Подробности в `RAG_PROJECT_README.md`

## Примеры вопросов

```
Как правильно настроить взгляд для видения ауры?
Какие упражнения для глаз рекомендуются?
Расскажи про биолокационную рамку
Как проводить диагностику энергоцентров?
```

## Производительность

На RTX 3090:
- **Embedding:** ~1000 документов/сек
- **Vector search:** <100ms
- **LLM inference:** ~20-50 tokens/sec (Gemma-3-27B)
- **Первая инициализация:** 2-3 минуты
- **Последующие запуски:** <5 секунд

## Лицензия

MIT

## Автор

Разработано в октябре 2025

Оптимизировано для RTX 3090 и LM Studio
